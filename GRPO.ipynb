{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GRPO"
      ],
      "metadata": {
        "id": "Vr6HI0oPXLr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.0005\n",
        "eps_clip = 0.1\n",
        "K_epoch = 3\n",
        "num_trajectories = 5  # Number of trajectories per initial state\n",
        "\n",
        "def layer_init(layer, std=1.41, bias_const=0.0):\n",
        "    torch.nn.init.orthogonal_(layer.weight, std)\n",
        "    torch.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer\n",
        "\n",
        "class GRPO(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GRPO, self).__init__()\n",
        "        self.data = []\n",
        "\n",
        "        self.fc1 = nn.Linear(4, 256)\n",
        "        self.fc_pi = nn.Linear(256, 2)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def pi(self, x, softmax_dim=0):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc_pi(x)\n",
        "        prob = F.softmax(x, dim=softmax_dim)\n",
        "        return prob\n",
        "\n",
        "    def put_data(self, transition):\n",
        "        self.data.append(transition)\n",
        "\n",
        "\n",
        "    def make_batch(self):\n",
        "        s, a, log_prob_a, advantages = zip(*self.data)\n",
        "        self.data = []\n",
        "        return (torch.tensor(np.array(s), dtype=torch.float),  # Fix applied here\n",
        "                torch.tensor(a).unsqueeze(1),\n",
        "                torch.tensor(log_prob_a).unsqueeze(1),\n",
        "                torch.tensor(advantages, dtype=torch.float).unsqueeze(1))\n",
        "\n",
        "\n",
        "    def train_net(self):\n",
        "        \"\"\" Perform the GRPO training update with PPO clipping \"\"\"\n",
        "        s, a, log_prob_a, advantages = self.make_batch()\n",
        "        for _ in range(K_epoch):\n",
        "            pi = self.pi(s, softmax_dim=1)\n",
        "            pi_a = pi.gather(1, a)\n",
        "            ratio = torch.exp(torch.log(pi_a) - log_prob_a)  # a/b == exp(log(a)-log(b))\n",
        "\n",
        "            surr1 = ratio * advantages\n",
        "            surr2 = torch.clamp(ratio, 1 - eps_clip, 1 + eps_clip) * advantages\n",
        "\n",
        "            loss = -torch.min(surr1, surr2)  # Clipped policy loss\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "def generate_trajectory(env, policy, seed):\n",
        "    \"\"\" Generate a single trajectory from the environment using a fixed seed \"\"\"\n",
        "    s, _ = env.reset(seed = seed)\n",
        "    s = np.array(s, dtype=np.float32)\n",
        "    terminated, truncated = False, False\n",
        "    log_probs = []\n",
        "    rewards = []\n",
        "    states = []\n",
        "    actions = []\n",
        "\n",
        "    while not (terminated or truncated):\n",
        "        prob = policy.pi(torch.from_numpy(s).float())\n",
        "        m = Categorical(prob)\n",
        "        a = m.sample()\n",
        "        s_prime, r, terminated, truncated, _ = env.step(a.item())\n",
        "\n",
        "        log_probs.append(torch.log(prob[a]))\n",
        "        rewards.append(r)\n",
        "        states.append(s)\n",
        "        actions.append(a.item())\n",
        "        s = np.array(s_prime, dtype=np.float32)\n",
        "\n",
        "    return states, actions, log_probs, rewards\n",
        "\n",
        "## Main\n",
        "env = gym.make('CartPole-v1')\n",
        "policy = GRPO()\n",
        "score = 0.0\n",
        "log_each_iteration = 20  # Defines how often we log (based on iterations)\n",
        "total_episodes = 0  # Tracks the total number of episodes played\n",
        "\n",
        "for iteration in range(1000):\n",
        "    seed = iteration\n",
        "    trajectories = [generate_trajectory(env, policy, seed) for _ in range(num_trajectories)]\n",
        "    total_episodes += num_trajectories\n",
        "\n",
        "    returns = np.array([sum(traj[3]) for traj in trajectories])\n",
        "    mean_r, std_r = returns.mean(), returns.std()\n",
        "    normalized_advantages = [(r - mean_r) / (std_r + 0.1) for r in returns]\n",
        "\n",
        "    for i in range(num_trajectories):\n",
        "        states, actions, log_probs, _ = trajectories[i]\n",
        "        adv = normalized_advantages[i]\n",
        "        for j in range(len(states)):\n",
        "            policy.put_data((states[j], actions[j], log_probs[j].item(), adv))\n",
        "\n",
        "    policy.train_net()\n",
        "    score += returns.mean()\n",
        "\n",
        "    if total_episodes % (num_trajectories * log_each_iteration) == 0:\n",
        "        print(f\"Total episodes: {total_episodes}, avg score: {score / log_each_iteration:.2f}\")\n",
        "        score = 0.0\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "e1zc2QtBXNYl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}